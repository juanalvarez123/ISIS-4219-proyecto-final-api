{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1ZuZtCe1pIJZq3FDC-hikcIDmQkz3O8Ez","authorship_tag":"ABX9TyN7XFqnedaEzZe9JoQc2ZdX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZsYkoy15gnXT","executionInfo":{"status":"ok","timestamp":1669324782920,"user_tz":300,"elapsed":12367,"user":{"displayName":"Leonardo Ángel","userId":"14026995232087929991"}},"outputId":"14bf041c-4f75-4b09-8d58-00eabbfafd11"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Laboratorio MLT/Project"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ON7ghE95gc-u","executionInfo":{"status":"ok","timestamp":1669324803896,"user_tz":300,"elapsed":346,"user":{"displayName":"Leonardo Ángel","userId":"14026995232087929991"}},"outputId":"11b45816-7ecc-4c80-e006-3764ce34b63b"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Laboratorio MLT/Project\n"]}]},{"cell_type":"markdown","source":["# Using model on cloud"],"metadata":{"id":"gUQTJ4tX4rDG"}},{"cell_type":"code","source":["!pip install transformers -q"],"metadata":{"id":"Rg9wZh_7V3xg","executionInfo":{"status":"ok","timestamp":1669324350403,"user_tz":300,"elapsed":4161,"user":{"displayName":"Leonardo Ángel","userId":"14026995232087929991"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["from transformers import pipeline\n","qa_pipeline = pipeline(\n","    \"question-answering\",\n","    model=\"mrm8488/bert-multi-cased-finetuned-xquadv1\",\n","    tokenizer=\"mrm8488/bert-multi-cased-finetuned-xquadv1\"\n",")\n","\n","def predict(context, question):\n","  answer = qa_pipeline({\n","    'context': context,\n","    'question': question})\n","  return answer"],"metadata":{"id":"VBozRXLM4va6","executionInfo":{"status":"ok","timestamp":1669331116526,"user_tz":300,"elapsed":4169,"user":{"displayName":"Leonardo Ángel","userId":"14026995232087929991"}}},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":["# Exporting model"],"metadata":{"id":"nzWEKKzCgY-V"}},{"cell_type":"code","source":["from pathlib import Path\n","import transformers\n","from transformers.onnx import FeaturesManager\n","from transformers import AutoConfig, AutoTokenizer, AutoModelForQuestionAnswering\n","from transformers import AutoModelForQuestionAnswering\n","from transformers import AutoModel\n","\n","# load model and tokenizer\n","tokenizer_id=\"mrm8488/bert-multi-cased-finetuned-xquadv1\"\n","model_id=\"mrm8488/bert-multi-cased-finetuned-xquadv1\",\n","\n","feature = \"question-answering\"\n","tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n","model = AutoModelForQuestionAnswering.from_pretrained(tokenizer_id)\n","\n","model = BertModel.from_pretrained(\"mrm8488/bert-multi-cased-finetuned-xquadv1\")\n","\n","# load config\n","model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model, feature=feature)\n","onnx_config = model_onnx_config(model.config)\n","\n","# export\n","\"\"\"\n","onnx_inputs, onnx_outputs = transformers.onnx.export(\n","        model = model,\n","        config=onnx_config,\n","        opset=13,\n","        output=Path(\"trfs-model.onnx\")\n",")\"\"\"\n","model.save_pretrained(\"ltrfs-model.onnx\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yyUmgZN-XSU_","executionInfo":{"status":"ok","timestamp":1669329608711,"user_tz":300,"elapsed":13117,"user":{"displayName":"Leonardo Ángel","userId":"14026995232087929991"}},"outputId":"e80273c8-c19f-4e37-8804-a0878236dfd5"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at mrm8488/bert-multi-cased-finetuned-xquadv1 were not used when initializing BertModel: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["!pip install onnxruntime -q"],"metadata":{"id":"a9ft4DJ8hQWI","executionInfo":{"status":"ok","timestamp":1669324961183,"user_tz":300,"elapsed":9076,"user":{"displayName":"Leonardo Ángel","userId":"14026995232087929991"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["import onnx\n","import onnxruntime as rt"],"metadata":{"id":"9JO4R4BNhu53","executionInfo":{"status":"ok","timestamp":1669325845898,"user_tz":300,"elapsed":535,"user":{"displayName":"Leonardo Ángel","userId":"14026995232087929991"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["sess = rt.InferenceSession(\"trfs-model.onnx\", providers=rt.get_available_providers())\n","\n","print(\"input name='{}' and shape={}\".format(sess.get_inputs()[0].name, sess.get_inputs()[0].shape))\n","print(\"output name='{}' and shape={}\".format(sess.get_outputs()[0].name, sess.get_outputs()[0].shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mfx5sjZZkSMw","executionInfo":{"status":"ok","timestamp":1669325851016,"user_tz":300,"elapsed":303,"user":{"displayName":"Leonardo Ángel","userId":"14026995232087929991"}},"outputId":"7219235a-68d3-4921-c6e3-a29036128bf9"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["input name='input_ids' and shape=['batch', 'sequence']\n","output name='start_logits' and shape=['batch', 'sequence']\n"]}]},{"cell_type":"code","source":["text=\"Manuel Romero has been working hardly in the repository hugginface/transformers lately\",\n","question= \"Who has been working hard for hugginface/transformers lately?\""],"metadata":{"id":"YF-uf3az0Cu8","executionInfo":{"status":"ok","timestamp":1669329992924,"user_tz":300,"elapsed":312,"user":{"displayName":"Leonardo Ángel","userId":"14026995232087929991"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["tokenizer_id=\"mrm8488/bert-multi-cased-finetuned-xquadv1\"\n","tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n","\n","onnx_model = onnx.load(\"trfs-model.onnx\")\n","inputs = tokenizer(question, question, add_special_tokens=True, return_tensors='np')\n","# outputs = onnx_model.run(input_feed=dict(tokenizer(question, text, add_special_tokens=True, return_tensors='np')), output_names=None)\n","outputs = onnx_model.run(input_feed=dict(inputs), output_names=None)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":226},"id":"aeWKEmZEz2-_","executionInfo":{"status":"error","timestamp":1669330141172,"user_tz":300,"elapsed":4932,"user":{"displayName":"Leonardo Ángel","userId":"14026995232087929991"}},"outputId":"80bfd3b6-3c39-46e2-91e9-bee0eb9e24ff"},"execution_count":84,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-84-5a46e8c4cb7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'np'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# outputs = onnx_model.run(input_feed=dict(tokenizer(question, text, add_special_tokens=True, return_tensors='np')), output_names=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_feed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: run"]}]}]}